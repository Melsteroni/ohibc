---
title: 'OHIBC data prep: Livelihoods'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/ohibc/src/templates/ohibc_hdr1.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

library(raster)
library(sf)

source('~/github/ohibc/src/R/common.R')  ### an OHIBC specific version of common.R

scenario <- 'v2017'
goal     <- 'le'
dir_git  <- '~/github/ohibc'
dir_goal <- file.path(dir_git, 'prep', goal, scenario)
dir_spatial <- file.path(dir_git, 'prep/spatial')

dir_goal_anx <- file.path(dir_M, 'git-annex/bcprep', goal, scenario) 
dir_data_gl <- file.path(dir_M, 'git-annex/globalprep', '_raw_data')
dir_data_bc <- file.path(dir_M, 'git-annex/bcprep', '_raw_data')

library(provRmd); prov_setup()

### set up proj4string options: BC Albers and WGS84
p4s_bcalb <- c('bcalb' = '+init=epsg:3005')
p4s_wgs84 <- c('wgs84' = '+init=epsg:4326')

```

# Summary: OHIBC Livelihoods

This script prepares layers (species presence and species health) for Livelihoods goal in 
British Columbia's coastal regions.  

From Halpern et al. (2014) (OHI California Current):

>Livelihood sub-goal: As was done in the global analysis, coastal livelihoods is measured by two equally weighted sub-components, the number of jobs (j), which is a proxy for livelihood quantity, and the per capita average annual wages (g), which is a proxy for job quality. For jobs we used a no-net loss reference point whereas for wages we used a spatial comparison. 

For British Columbia, we do not currently have sector-specific employment and wage information.  As such we will analyze Livelihoods according to the model:

$x_{LIV} = \frac{j' + g'}{2}$

$j' = \frac{j_c / j_{ref}}{M_c / M_{ref}}$

where M is each region’s employment rate as a percent at current (c) and reference (ref) time periods, and:

$g' = \frac{g_c / g_{ref}}{W_c / W_{ref}}$

where W is each State’s average annual per capita wage at current (c) and reference (ref) time periods.

-----

# Data sources

* CIESEN gridded population of the world, population density v4
* Jobs data? wages data?

-----

# Methods

## Apportion BC census blocks to OHIBC regions

BC Census blocks do not line up well with OHIBC regions.  We will use the Gridded Population of the World population density dataset to determine proper weighting of census data within each region.

### Create shapefile of census blocks intersected with OHIBC regions.

Here we must use the unclipped OHIBC regions to capture inland extents of the BC census divisions.  Let's do it in BC Albers so the population density (after transforming) can easily be converted into populations per polygon.

``` {r census_vs_ohibc_rgns}

if(!file.exists(file.path(dir_goal, 'spatial', 'coastal_cens_dists.shp')) |
   !file.exists(file.path(dir_goal, 'spatial', 'ohibc_census_clip.shp'))) {

  ohibc_poly <- sf::read_sf(dsn = dir_spatial,
                            layer = 'ohibc_rgns_unclipped') %>%
    st_transform(3005)
  bc_census  <- sf::read_sf(dsn = file.path(dir_data_bc, 'le/census_blocks/gcd_000b11a_e'),
                            layer = 'gcd_000b11a_e') %>%
    filter(str_detect(PRNAME, 'British Columbia')) %>%
    st_transform(3005)
  # sf::st_crs(ohibc_poly); sf::st_crs(bc_census)
  
  ohibc_census <- sf::st_intersection(ohibc_poly, bc_census)
  
  # plot(ohibc_census)
  
  sf::write_sf(ohibc_census,
               dsn = file.path(dir_goal, 'spatial'),
               layer = 'ohibc_census_clip',
               driver = 'ESRI Shapefile')
  
  coastal_census <- bc_census %>%
    filter(CDUID %in% ohibc_census$CDUID)
  
  # plot(coastal_census)
  
  sf::write_sf(coastal_census,
               dsn = file.path(dir_goal), # dsn = file.path(dir_goal, 'spatial'),
               layer = 'coastal_cens_dists',
               driver = 'ESRI Shapefile')
  ### for some reason thinks the file already exists; though it doesn't have a problem
  ### writing over the ohibc_census_clip file that legitimately does exist.
}

```

### Determine population weights for census districts to OHIBC regions

For census districts that cross OHIBC region boundaries (to other regions, or outside the region), use population density rasters to determine how to divide population into regions.  Population-density rasters (people per 1 km^2^) will be reprojected to BC Albers and 1-km grid cells for easy population calculations.  Zonal sums will determine total and region-specific populations.

Note that when the resulting weights were calculated they were essentially unchanged from year to year (and the tiny changes are most likely due to rounding rather than actual changes in the data).  The pop density data is modeled, so likely it is a simple extrapolation from year to year without changing the distribution of population.  For determining weights we will simply choose a single year, 2015.

``` {r trim_pop_dens_rasters}

# pop_dens_years <- seq(2000, 2020, 5)
pop_dens_years <- 2015 
### see note below; relative density appears constant, so one year is adequate

bc_dens_files <- file.path(dir_goal, 'spatial', sprintf('pop_dens_%s.tif', pop_dens_years))

# if(any(!file.exists(bc_dens_files))) {
  ### use this to get bounding box for cropping
  bb <- sf::read_sf(dsn = file.path(dir_goal, 'spatial'),
                                      layer = 'coastal_cens_dists') %>%
    sf::st_transform(4326) %>%
    sf::st_bbox() 
  bb_ext <- raster::extent(c(bb[1], bb[3], bb[2], bb[4]))
  
  base_rast <- raster::raster(file.path(dir_spatial, 'raster/ohibc_rgn_raster_1000m.tif'))
  
  pop_dens_files <- file.path(dir_data_gl, 'CIESEandCIAT_population',
                              sprintf('d2017/gpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-%s', pop_dens_years),
                              sprintf('gpw-v4-population-density-adjusted-to-2015-unwpp-country-totals_%s.tif', pop_dens_years))
  pop_dens <- raster::stack(pop_dens_files) %>%
    raster::crop(bb_ext) %>%
    raster::projectRaster(base_rast)
  
  # plot(pop_dens[[1]])
  writeRaster(pop_dens, bc_dens_files, bylayer = TRUE, overwrite = TRUE)
# }

```


``` {r get_pop_totals_for_census_dists}

cd_pop_totals_file <- file.path(dir_goal, 'int/pop_total_census_dists.csv')

# if(!file.exists(cd_pop_totals_file)) {
  census_dists_poly <- sf::read_sf(dsn = file.path(dir_goal, 'spatial'),
                             layer = 'coastal_cens_dists') %>%
    mutate(CDUID = as.integer(CDUID))
  base_rast <- raster::raster(file.path(dir_spatial, 'raster/ohibc_rgn_raster_1000m.tif'))
  cd_rast <- fasterize::fasterize(census_dists_poly, base_rast, field = 'CDUID')
  
  bc_dens_files <- file.path(dir_goal, 'spatial', sprintf('pop_dens_%s.tif', pop_dens_years))
  pop_dens <- raster::stack(bc_dens_files)
  values(pop_dens)[values(pop_dens) < 0] <- 0
  
  cd_totpop <- vector('list', length = length(bc_dens_files))
  for(i in seq_along(bc_dens_files)) {
    cd_totpop[[i]] <- raster::zonal(pop_dens[[i]], cd_rast, fun = 'sum') %>%
      as.data.frame()
  }
  names(cd_totpop) <- str_extract(bc_dens_files, '[0-9]{4}(?=[.csv$])')
  
  cd_totpop_df <- bind_rows(cd_totpop, .id = 'year') %>%
    rename(dist_id   = zone,
           pop_total = sum) %>%
    mutate(pop_total = round(pop_total))

  cd_totpop_df <- cd_totpop_df %>%
    left_join(census_dists_poly %>%
                as.data.frame() %>%
                select(dist_id = CDUID, dist_name = CDNAME),
              by = c('dist_id'))
  
  # ggplot(cd_totpop, aes(x = year, y = sum, color = zone)) +
  #   geom_line(aes(group = zone))
  
  write_csv(cd_totpop_df, cd_pop_totals_file)
# }

```

``` {r get_pop_by_census_dist_per_ohibc_rgn}

cd_pop_by_ohibc_file <- file.path(dir_goal, 'int/pop_census_by_ohibc.csv')

# if(!file.exists(cd_pop_by_ohibc_file)) {
  census_by_ohibc_poly <- sf::read_sf(dsn = file.path(dir_goal, 'spatial'),
                             layer = 'ohibc_census_clip') %>%
    mutate(CDUID = as.integer(CDUID),
           ohibc_cduid = CDUID + rgn_id * 10000)
  base_rast <- raster::raster(file.path(dir_spatial, 'raster/ohibc_rgn_raster_1000m.tif'))
  cd_ohibc_rast <- fasterize::fasterize(census_by_ohibc_poly, base_rast, field = 'ohibc_cduid')
  
  bc_dens_files <- file.path(dir_goal, 'spatial', sprintf('pop_dens_%s.tif', pop_dens_years))
  pop_dens <- raster::stack(bc_dens_files)
  values(pop_dens)[values(pop_dens) < 0] <- 0
  
  cd_ohibcpop <- vector('list', length = length(bc_dens_files))
  for(i in seq_along(bc_dens_files)) {
    cd_ohibcpop[[i]] <- raster::zonal(pop_dens[[i]], cd_ohibc_rast, fun = 'sum') %>%
      as.data.frame()
  }
  names(cd_ohibcpop) <- str_extract(bc_dens_files, '[0-9]{4}(?=[.csv$])')
  

  cd_ohibcpop_df <- bind_rows(cd_ohibcpop, .id = 'year') %>%
    mutate(rgn_id       = floor(zone / 10000),
           dist_id  = zone - (rgn_id * 10000),
           pop_rgn_dist = round(sum)) %>%
    select(-zone, -sum)
  
  cd_ohibcpop_df <- cd_ohibcpop_df %>%
    left_join(census_by_ohibc_poly %>%
                as.data.frame() %>%
                select(rgn_id, dist_id = CDUID, rgn_name, dist_name = CDNAME),
              by = c('rgn_id', 'dist_id'))
  
  # ggplot(cd_ohibcpop, aes(x = year, y = sum, color = zone)) +
  #   geom_line(aes(group = zone))
  
  write_csv(cd_ohibcpop_df, cd_pop_by_ohibc_file)
  
# }

```

# NEEDED????
``` {r combine_pop_results_for_weights}
# 
# cd_pop_tots <- read_csv(file.path(dir_goal, 'int/pop_total_census_dists.csv'))
# cd_pop_ohibc <- read_csv(file.path(dir_goal, 'int/pop_census_by_ohibc.csv'))
# 
# cd_pop_weights <- cd_pop_ohibc %>%
#   left_join(cd_pop_tots, by = c('year', 'dist_id', 'dist_name')) %>%
#   mutate(cd_pop_wt = round(pop_rgn_dist / pop_total, 5)) %>%
#   filter(year == max(year)) %>%
#   select(-year)
# 
# # check_sums <- cd_pop_weights %>%
# #   group_by(year, dist_id) %>%
# #   mutate(summed_dist = sum(pop_rgn_dist),
# #          bust = round(summed_dist / pop_total, 4)) %>%
# #   select(dist_id, bust)
# # 
# # x <- census_by_ohibc_poly %>%
# #   left_join(check_sums, by = c('CDUID' = 'dist_id'))
# # 
# # plot(census_dists_poly %>% select(CDUID, geometry))
# # plot(x %>% select(bust, geometry))
# 
# ### Honestly, the proportions are all essentially linear, unchanging from
# ### year to year.  No sense in complicating things; just use a single
# ### year to determine weightings.
# 
# write_csv(cd_pop_weights, file.path(dir_goal, 'int', 'census_ohibc_weights.csv'))

```

## Read in census data

Due to different formats in 2001, 2006, and 2011, here we read in the various .csvs, filter to appropriate categories and regions, and combine into a single dataframe for later use.

``` {r read_census_data}

census_data_dir <- file.path(dir_data_bc, 'le', 'census_data')

census_files <- list.files(census_data_dir, include.dirs = FALSE, 
                           pattern = '.csv$',
                           full.names = TRUE)

cens_file_2011 <- census_files[str_detect(census_files, '2011')]
cens_file_0106 <- census_files[!str_detect(census_files, '2011')]

census_0106 <- lapply(cens_file_0106, FUN = function(x) {
    tmp_df <- read_csv(x, skip = 1) %>%
      ### all cols in same order, but names change; standardize them here:
      setNames(c('category', 'subcat', 'dist_tot', 'dist_m', 'dist_f', 'bc_tot', 'bc_m', 'bc_f')) %>%
      mutate(file_name = basename(x))
    return(tmp_df)
  }) %>%
  bind_rows()

census_0106_clean <- census_0106 %>%
  ### use filename to determine district and year
  mutate(district = str_replace_all(tolower(file_name), 'download|.csv$', ''),
         district = str_replace_all(district, '[^a-z]', ''),
         year = str_match(file_name, '[0-9]{4}'),
         year = as.integer(year)) %>%
  select(-file_name)

### for 2011, select columns and rename to match data for '01 and '06
census_2011 <- read_csv(cens_file_2011, skip = 1) %>%
  setNames(tolower(names(.))) %>%
  filter(str_detect(prov_name, 'British Columbia')) %>%
  select(district = cd_name, 
         category = topic, subcat = characteristic,
         dist_tot = total,
         dist_m   = male,
         dist_f   = female) %>%
  mutate(year = 2011) %>%
  mutate(district = str_replace_all(tolower(district), '[^a-z]', '')) %>%
  distinct()

census_all_raw <- census_0106_clean %>%
  bind_rows(census_2011)

write_csv(census_all_raw, file.path(dir_goal, 'int', 'census_all_raw.csv'))

```

``` {r create_dist_name_lookup, eval = FALSE}

# census_all_raw <- read_csv(file.path(dir_goal, 'int', 'census_all_raw.csv'))
# 
# dist_list_0106 <- census_all_raw %>%
#   filter(year %in% c(2001, 2006)) %>%
#   .$district %>% unique()
# dist_list_2011 <- census_all_raw %>%
#   filter(year %in% c(2011)) %>%
#   .$district %>%
#   tolower() %>% str_replace_all('[^a-z]', '') %>%
#   unique()
# dist_lookup <- data.frame(dist_include = dist_list_0106) %>%
#   mutate(dist_2011 = rep(list(dist_list_2011), times = nrow(.))) %>%
#   unnest(dist_2011) %>%
#   filter(str_detect(dist_2011, dist_include))
# 
# dist_lookup_from_spatial <- sf::read_sf(file.path(dir_goal, 'spatial'),
#                                     'ohibc_census_clip') %>%
#   as.data.frame() %>%
#   setNames(tolower(names(.))) %>%
#   select(cduid, dist_2011 = cdname) %>%
#   distinct() %>%
#   mutate(dist_2011 = str_replace_all(tolower(dist_2011), '[^a-z]', '')) %>%
#   full_join(dist_lookup, by = c('dist_2011'))
# 
# write_csv(dist_lookup_from_spatial, file.path(dir_goal, 'raw', 'dist_name_lookup_raw.csv'))

```

``` {r clean_up_dist_stats}

dist_ids <- read_csv(file.path(dir_goal, 'raw', 'dist_ids_lookup_clean.csv')) %>%
  gather(dataset, district, contains('dist_20'))

### Chop down full census dataset to coastal regions & categories of interest; then
### fix district names and IDs.
### Because the fields of interest are not counts, but averages, we can split
### the Comox/Strathcona blocks into Comox and Strathcona.  Note that the
### difference in values for 2011 income and employment can give us more
### of an idea of how these split between the two districts, but they are
### very close so it would provide negligible impact on final score.
category_valid <- c('labour force', 'income', 'household')
census_coast <- read_csv(file.path(dir_goal, 'int', 'census_all_raw.csv')) %>%
  filter(str_detect(tolower(category), paste(category_valid, collapse = '|'))) %>%
  ### convoluted... first, add district IDs by district name; then...
  inner_join(dist_ids, by = 'district') %>%
  ### ...clear out district name, then...
  select(-dataset, -district) %>%
  ### ...reattach *current* district names by ID.
  left_join(dist_ids %>% filter(dataset == 'dist_2011') %>% select(-dataset),
            by = 'dist_id') %>%
  distinct()


### isolate employment stats
census_employ <- census_coast %>%
  filter(str_detect(tolower(subcat),   'employment rate')) %>%
  select(year, district, dist_id, empl_type = subcat, empl_rate = dist_tot) %>%
  distinct()

write_csv(census_employ, file.path(dir_goal, 'int', 'census_employment.csv'))


# ### isolate total population stats
# pop_fld_valid <- c('population in [0-9]{4}',
#                    'total population in private households by citizenship')
#   ### note 'Total population' is less than 'Population in 2006' (e.g.)
# census_pop <- census_coast %>%
#   filter(str_detect(tolower(subcat),   paste(pop_fld_valid, collapse = '|'))) %>%
#   filter(!(year %in% c(2001, 2006) & !str_detect(subcat, as.character(year)))) %>%
#   select(year, district, dist_pop = dist_tot) %>%
#   distinct()
# 
# write_csv(census_pop, file.path(dir_goal, 'int', 'census_pop.csv'))


### isolate income stats
income_fld_valid <- c('median household income.*all',
                   'median income.*all.*household',
                   'median household total income')
census_income <- census_coast %>%
  filter(str_detect(tolower(subcat),   paste(income_fld_valid, collapse = '|'))) %>%
  select(year, district, dist_id, income_cat = subcat, dist_median = dist_tot) %>%
  distinct()

write_csv(census_income, file.path(dir_goal, 'int', 'census_income.csv'))

```

## Aggregate census data to OHIBC region by population weight

Income and employment values at census district level are aggregated to the OHIBC region level, using population weighting of each CD within each OHIBC region to determine a weighted mean.

``` {r calc_income_layers}
rgn_df <- read_csv(file.path(dir_goal, 'int', 'pop_census_by_ohibc.csv')) %>%
  filter(year == max(year)) %>%
  select(-year)

income_df <- read_csv(file.path(dir_goal, 'int', 'census_income.csv')) %>%
  left_join(rgn_df, by = 'dist_id') %>%
  filter(pop_rgn_dist > 0)

income_wt_mean <- income_df %>%
  group_by(year, rgn_id) %>%
  mutate(dist_median = as.numeric(dist_median)) %>%
  summarize(median_income = sum(dist_median * pop_rgn_dist) / sum(pop_rgn_dist),
            median_income = round(median_income, 2),
            pop_rgn = sum(pop_rgn_dist)) %>%
  ungroup()

write_csv(income_wt_mean, file.path(dir_goal, 'output', 'le_income.csv'))

DT::datatable(income_wt_mean, caption = 'Median income')

```

``` {r calc_employment_layers}
rgn_df <- read_csv(file.path(dir_goal, 'int', 'pop_census_by_ohibc.csv')) %>%
  filter(year == max(year)) %>%
  select(-year)

employ_df <- read_csv(file.path(dir_goal, 'int', 'census_employment.csv')) %>%
  left_join(rgn_df, by = 'dist_id') %>%
  filter(pop_rgn_dist > 0)

employ_wt_mean <- employ_df %>%
  group_by(year, rgn_id, empl_type) %>%
  summarize(mean_rate = sum(empl_rate * pop_rgn_dist) / sum(pop_rgn_dist),
            mean_rate = round(mean_rate / 100, 5),
            pop_rgn = sum(pop_rgn_dist)) %>%
  ungroup()

empl_layer <- employ_wt_mean %>%
  filter(!str_detect(empl_type, 'Unemployment')) %>%
  select(-empl_type) %>%
  mutate(employment_rate = mean_rate)

unempl_layer <- employ_wt_mean %>%
  filter(str_detect(empl_type, 'Unemployment')) %>%
  select(-empl_type) %>%
  mutate(unemployment_rate = mean_rate)

write_csv(empl_layer,   file.path(dir_goal, 'output', 'le_employment.csv'))
write_csv(unempl_layer, file.path(dir_goal, 'output', 'le_unemployment.csv'))

DT::datatable(empl_layer, caption = 'Employment rate')
DT::datatable(unempl_layer, caption = 'Unemployment rate')

```

***

``` {r provenance, results = 'asis'}

prov_wrapup()

```
